![image](https://github.com/user-attachments/assets/961e3d26-38f6-4daa-90b2-7a4861729fc3)# Introduction
As AI models evolve, detecting AI-generated text becomes increasingly challenging. Advanced models, especially when multiple LLMs collaborate in generating content, can effectively evade detection systems, mimicking human writing with high fluency. Traditional methods often fail to distinguish between AI and human-authored text. To address these challenges, we propose an enhanced detection framework that excels even against sophisticated evasion attempts. First, we leverage open source models (Gemma2-9b, Qwen2-7b, GPT-2-Large) to calculate token loss and perplexity, strengthening detection accuracy. Second, we incorporate semantic, part-of-speech (PoS), and bigram features to capture subtle human-like nuances. Our method demonstrates superior performance in identifying AI-generated content, even when crafted using Multi-LLM techniques for evasion. This work offers new insights into robust content analysis and improves the detection of AI-generated text.
<p align="center">
  <img src="https://github.com/user-attachments/assets/b88a562e-5615-4a86-a600-305204a80c46" alt="The text generation process of multi-LLM system">
  <br>
  The text generation process of multi-LLM system
</p>

# DetevaGPT
To address the challenges of sentence-level AI-generated text detection, we integrate semantic features, writing style features, and perplexity as foundational features for detecting AI-generated content. At the same time, we incorporate training data generated by multiple LLMs into the training set. On one hand, this approach allows us to validate the effectiveness of our proposed method in detecting content generated by newer, larger models. On the other hand, it enables us to verify that text generated by multiple LLMs is closer to human writing and thus more difficult to detect, which contributes to train a more robust detection model. We refer to our method as DetevaGPT.Our DetevaGPT model is composed of three parts: Feature extraction, feature encoding, and sequence labeling.
<p align="center">
  <img src="https://github.com/user-attachments/assets/558b39a9-858d-48b9-9fda-12adaea8d35c" alt="Model structure of DetevaGPT">
  <br>
  Model structure of DetevaGPT
</p>

# Performance
All the values listed in our table are F1 scores scores to consider the overall performance. 
| Column 1 Header | Column 2 Header | Column 3 Header | Column 4 Header | Column 5 Header | Column 6 Header |
| --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| Row 1, Column 1 | Row 1, Column 2 | Row 1, Column 3 | Row 1, Column 4 | Row 1, Column 5 | Row 1, Column 6 |
| Row 2, Column 1 | Row 2, Column 2 | Row 2, Column 3 | Row 2, Column 4 | Row 2, Column 5 | Row 2, Column 6 |
| Row 3, Column 1 | Row 3, Column 2 | Row 3, Column 3 | Row 3, Column 4 | Row 3, Column 5 | Row 3, Column 6 |

| Column 1 Header | Column 2 Header | Column 3 Header | Column 4 Header | Column 5 Header | Column 6 Header |
| --------------- | --------------- | --------------- | --------------- | --------------- | --------------- |
| Row 1, Column 1 | Row 1, Column 2 | Row 1, Column 3 | Row 1, Column 4 | Row 1, Column 5 | Row 1, Column 6 |
| Row 2, Column 1 | Row 2, Column 2 | Row 2, Column 3 | Row 2, Column 4 | Row 2, Column 5 | Row 2, Column 6 |
| Row 3, Column 1 | Row 3, Column 2 | Row 3, Column 3 | Row 3, Column 4 | Row 3, Column 5 | Row 3, Column 6 |

# Training Loss vs Epochs

![image](https://github.com/user-attachments/assets/4432ed53-158d-4430-9a7c-0ba243278d2d)

# UMAP comparison of clustering effects of SeqXGPT and DetevaGPT Table
![image](https://github.com/user-attachments/assets/7f39cc66-dda1-4d58-b496-2b5329aa1199)  ![image](https://github.com/user-attachments/assets/dd556991-2772-43f8-a358-ae4b154999fb)



